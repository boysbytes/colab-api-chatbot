{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boysbytes/colab-chatbot/blob/main/colab_rag_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "TGuHTQAAZjdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama\n",
        "import os\n",
        "from google.colab import drive\n",
        "import ollama\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ollama_models_path = '/content/drive/MyDrive/ollama_models'\n",
        "\n",
        "if not os.path.exists(ollama_models_path):\n",
        "    os.makedirs(ollama_models_path)\n",
        "    print(f\"Created directory: {ollama_models_path}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {ollama_models_path}\")\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install -U langchain chromadb sentence-transformers gradio pypdf langchain-community"
      ],
      "metadata": {
        "id": "M9rk4XFQtuZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Ollama and model directory"
      ],
      "metadata": {
        "id": "t0tYBO3m3eum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama in a separate process\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "\n",
        "running = subprocess.run([\"pgrep\", \"-a\", \"ollama\"], capture_output=True, text=True)\n",
        "print(\"Ollama Status:\", running.stdout if running.stdout else \"Ollama is NOT running!\")\n",
        "\n",
        "!rm -rf ~/.ollama/models\n",
        "!ln -s /content/drive/MyDrive/ollama_models ~/.ollama/models\n"
      ],
      "metadata": {
        "id": "R2C4RvcTZmMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the model to Google Drive"
      ],
      "metadata": {
        "id": "mjQ2hiXBbDek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_info_path = '/content/drive/MyDrive/ollama_models/manifests/registry.ollama.ai/library/smollm2/latest'\n",
        "\n",
        "if not os.path.exists(model_info_path):\n",
        "    !ollama pull smollm2\n",
        "else:\n",
        "    print(\"Model already exists. Skipping download.\")"
      ],
      "metadata": {
        "id": "Z4szQvG6bE5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement RAG\n",
        "\n",
        "1. Create a `rag_data` folder in your Google Drive.\n",
        "2. Copy your notes (in PDF, TXT, DOCX) into the folder.\n",
        "3. Run the code cell below.\n",
        "\n",
        "  > This part may take a while to complete."
      ],
      "metadata": {
        "id": "dfMkzvxf_S0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG implementation\n",
        "\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Load Documents (Support for multiple file types)\n",
        "data_folder = '/content/drive/MyDrive/rag_data'\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(data_folder):\n",
        "    file_path = os.path.join(data_folder, filename)\n",
        "    try:\n",
        "        if filename.endswith('.txt'):\n",
        "            loader = TextLoader(file_path)\n",
        "        elif filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx') or filename.endswith('.csv'):\n",
        "            loader = UnstructuredFileLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "# 2. Split Documents (Dynamic chunk size based on document type)\n",
        "chunk_size = 500\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# 3. Create Embeddings and Vector Store\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n",
        "# db.persist()"
      ],
      "metadata": {
        "id": "yfWGNs_Zk_l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refine your prompt template"
      ],
      "metadata": {
        "id": "8BJ8nk2QeYLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a helpful AI tutor for a 14-year-old student. Use the context provided to answer the student's questions clearly and concisely.\n",
        "\n",
        "If the answer to the question is not found in the context, say \"I don't know.\" Avoid making up information and avoid using your prior knowledge.\n",
        "\n",
        "Maintain context across multiple turns of conversation.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"RAG setup complete.\")"
      ],
      "metadata": {
        "id": "qi5PDjQeea_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the chatbot function"
      ],
      "metadata": {
        "id": "cRl-Q6iKW6j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_message, chat_history):\n",
        "    # Retrieve relevant documents with reranking logic\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 relevant chunks\n",
        "\n",
        "    try:\n",
        "        relevant_documents = retriever.get_relevant_documents(user_message)\n",
        "        # Rerank results based on relevance score (if available)\n",
        "        relevant_documents.sort(key=lambda doc: doc.metadata.get('relevance_score', 0), reverse=True)\n",
        "\n",
        "        # Format the context from top-ranked documents\n",
        "        context = \"\\n\".join([doc.page_content for doc in relevant_documents])\n",
        "\n",
        "        # Include chat history in the prompt for multi-turn conversation support\n",
        "        history_context = \"\\n\".join([f\"{role}: {content}\" for role, content in chat_history])\n",
        "        prompt = PROMPT_TEMPLATE.format(context=context, question=user_message) + f\"\\n\\nConversation History:\\n{history_context}\"\n",
        "\n",
        "        # Generate response using Qwen2.5 model via Ollama\n",
        "        # response = ollama.chat(model=\"qwen2.5:0.5b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        response = ollama.chat(model=\"smollm2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        bot_message = response[\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        bot_message = f\"Error: {e}\"\n",
        "\n",
        "    # Update chat history and return it along with the bot's response\n",
        "    chat_history.append((\"User\", user_message))\n",
        "    chat_history.append((\"Chatbot\", bot_message))\n",
        "    return chat_history, chat_history"
      ],
      "metadata": {
        "id": "L_v3QojxXBjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the chatbot interface"
      ],
      "metadata": {
        "id": "TtQLB7dmXLz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as chatui:\n",
        "    gr.Markdown(\"# RAG-Enabled Chatbot\")\n",
        "\n",
        "    # Improved Chatbot Display\n",
        "    chatbot_display = gr.Chatbot(\n",
        "        height=300,  # Adjust height as needed\n",
        "    )\n",
        "\n",
        "    # User Input with Styling\n",
        "    user_input_box = gr.Textbox(\n",
        "        label=\"Your Message\",\n",
        "        placeholder=\"Ask me anything!\",\n",
        "        container=True,\n",
        "        scale=7,\n",
        "    )\n",
        "\n",
        "    # Submit Button with Styling\n",
        "    send_button = gr.Button(\n",
        "        \"Send\",\n",
        "        variant=\"primary\",  # More visually prominent\n",
        "        scale=1,\n",
        "    )\n",
        "\n",
        "    # Clear Button\n",
        "    clear_button = gr.ClearButton([user_input_box, chatbot_display], value=\"Clear\")\n",
        "\n",
        "    # State for Conversation History\n",
        "    chat_history = gr.State([])\n",
        "\n",
        "    # Define Functionality\n",
        "    send_button.click(\n",
        "        chatbot_response,\n",
        "        inputs=[user_input_box, chat_history],\n",
        "        outputs=[chatbot_display, chat_history],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Launch the chatbot UI with sharing enabled\n",
        "chatui.launch(share=True)\n"
      ],
      "metadata": {
        "id": "sc9yje18XN5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oL1sH-hiEQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}